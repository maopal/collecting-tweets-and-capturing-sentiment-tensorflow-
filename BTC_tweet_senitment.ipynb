{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BTC tweet senitment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBTOkT5OB0mLZX6KU7ggdD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/palemao/bitcoin-price-prediction/blob/main/BTC_tweet_senitment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1ibxLLmbuHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86d4e1d-2b06-4924-c51a-122cf2eefe36"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "!pip install -U keras-tuner\r\n",
        "import kerastuner as kt\r\n",
        "import IPython\r\n",
        "!pip install pyyaml h5py \r\n",
        "from keras import regularizers "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 21.3MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 19.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 11.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 9.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (1.0.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp36-none-any.whl size=78939 sha256=83bbf14c6681192f766278c80f112238c194d60b957ca424cb5ed4dae2a80318\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15358 sha256=95cb8097b301110ee3412159b7584dd4833f903438517106213fc8c51668c5bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW8LVn_EVnAj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "459d1875-23dc-41e3-b335-ff1c1f7fb8ab"
      },
      "source": [
        "df= pd.read_csv('/uniquetweets.csv') \r\n",
        "df.head()\r\n",
        "print(len(df))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2Bvwuyg8QHD"
      },
      "source": [
        "### Definitions\r\n",
        "\r\n",
        "Vocab_size = The possible number of words \r\n",
        "\r\n",
        "embedding_dim = The vector size for each word which represents how similar or different words are from each other\r\n",
        "\r\n",
        "max_length = Maximum Tweet Length\r\n",
        "\r\n",
        "padding_type = Tweets shorter than 100 words are padded with 0 after the last word to ensure all tensors are the same length before being fed into neural network\r\n",
        "\r\n",
        "oov_tok = placeholder for words that could not be encoded\r\n",
        "\r\n",
        "Training index = Creates an index number 80% of the length of the entire data set, this number is used to seperate the dataset into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_lEFn0iiDku"
      },
      "source": [
        "vocab_size = 10000\r\n",
        "embedding_dim = 16\r\n",
        "max_length = 100\r\n",
        "trunc_type='post'\r\n",
        "padding_type='post'\r\n",
        "oov_tok = \"<OOV>\"\r\n",
        "training_index = int(len(df)*0.8)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp5aYMH79kqn"
      },
      "source": [
        "Creating test and train datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxuuH8OgYJBp",
        "outputId": "24a9e634-3d84-44fe-8ce1-de1023df78f5"
      },
      "source": [
        "labels = np.asarray(df['sent_score'])\r\n",
        "\r\n",
        "test_tweets = np.asarray(df[training_index:]['Tweet'].tolist())\r\n",
        "test_labels = np.asarray(labels[training_index:])\r\n",
        "\r\n",
        "tweets = np.asarray(df[:training_index]['Tweet'].tolist())\r\n",
        "labels = np.asarray(labels[:training_index])\r\n",
        "\r\n",
        "\r\n",
        "print(\"Original Labels \\n\", labels[25:40])\r\n",
        "\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Labels \n",
            " [0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI6G2gSZ-VDB"
      },
      "source": [
        "Takes words from tweets and encodes them into numbers, placing \"<OOV>\" for unknown words:\r\n",
        "\r\n",
        "Each word has a unqiue number now, print word_index to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeHARlzZckb6"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\r\n",
        "tokenizer.fit_on_texts(tweets)\r\n",
        "word_index= tokenizer.word_index"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT8gBnSn_fBI"
      },
      "source": [
        "1. Encoded/tokenised words can be used to make \"token sentences\" from tweets.\r\n",
        "\r\n",
        "2. `padding_sequences()` adds 0's after tweets that are less than 100 words and truncates tweets more than 100 words. Therefore each encoded tweet is the same length of 100\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_npnjgfebI02",
        "outputId": "473b4765-fcf4-4380-fe92-bd9361a5d3ad"
      },
      "source": [
        "#training padded\r\n",
        "sequences = tokenizer.texts_to_sequences(tweets)\r\n",
        "padded = np.array(pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type))\r\n",
        "print(tweets[123])\r\n",
        "print(padded[123])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Ronald_vanLoon \" Ronald_vanLoon: Raiden Network #IoT Demo \n",
            "by:brainbot_tech |\n",
            "\n",
            "#InternetofThings #AI #Artificialintel…\n",
            "[5885 5886 5885 5886 5887  203  425 1543   43 8767  354 1544  414 8768\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9W4t1e3XoVZ6",
        "outputId": "956f9cf5-a59c-4498-a769-f13a1c5e0821"
      },
      "source": [
        "#testing padded\r\n",
        "testing_seq = tokenizer.texts_to_sequences(test_tweets)\r\n",
        "testing_padded = np.asarray(pad_sequences(testing_seq, maxlen=max_length ,padding=padding_type, truncating=trunc_type))\r\n",
        "print(test_tweets[123])\r\n",
        "print(testing_padded[123])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Puerto Rico creates a working group to review blockchain projects - \n",
            "[3711 4398 1955    6  692  379    4  980   10  766    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u76ImBfvBb3M"
      },
      "source": [
        "Define BinaryCrossentropy() loss function used for binary classification tasks. \r\n",
        "\r\n",
        "Define early stopping callback to prevent over fitting of model and to save time when using trial and error to create models. This callback stops training if there is an insignificant reduction/increase in validation loss, there is a patience of 2, to provide some leeway "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wj27lBXYxbHV"
      },
      "source": [
        "tf.keras.losses.BinaryCrossentropy(\r\n",
        "    from_logits=False, label_smoothing=0, reduction=\"auto\", name=\"binary_crossentropy\"\r\n",
        ")\r\n",
        "\r\n",
        "callback  = tf.keras.callbacks.EarlyStopping(\r\n",
        "    monitor=\"val_loss\",\r\n",
        "    min_delta=0,\r\n",
        "    patience=2,\r\n",
        "    verbose=1,\r\n",
        "    mode=\"auto\",\r\n",
        "    baseline=None,\r\n",
        "    restore_best_weights=False,\r\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHvxVxXgClyd"
      },
      "source": [
        "Define Hyper-Model, here using Gaussian Statistics we can use **baysian optimisation** method, where combination of hyperparamters are tested based on Gaussian Distribution which highlights where the next best combination is likley to be based on previous trials. This method has shown greater results compared to other hyperparameter optimastion methods such as grid search or hyperband."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hAlrj0sh0No"
      },
      "source": [
        "def model_builder(hp):  #Things to try: 1. drop out choice  2.Number of layers \r\n",
        "  model = keras.Sequential()\r\n",
        "  \r\n",
        "  ##first layer\r\n",
        "  hp_units1 = hp.Int('units1', min_value = 32, max_value = 512, step = 32)\r\n",
        "  dp1 = hp.Choice('dp1', values = [0.25,0.5,0.75]) \r\n",
        "\r\n",
        "  #second layer\r\n",
        "  hp_units2 = hp.Int('units2', min_value = 32, max_value = 512, step = 32)\r\n",
        "  dp2 = hp.Choice('dp2', values = [0.25,0.5,0.75])\r\n",
        " \r\n",
        "  #third layer\r\n",
        "  hp_units3 = hp.Int('units3', min_value = 32, max_value = 512, step = 32)\r\n",
        "  dp3 = hp.Choice('dp3', values = [0.25,0.5,0.75])\r\n",
        " \r\n",
        "  model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)) #embedding layer\r\n",
        " \r\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hp_units1,  return_sequences=True))) #layer 1\r\n",
        "  tf.keras.layers.Dropout(dp1),\r\n",
        " \r\n",
        "  model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(hp_units2))) # layer2\r\n",
        "  tf.keras.layers.Dropout(dp2),\r\n",
        " \r\n",
        "  model.add(keras.layers.Dense(units=hp_units3, activation='relu')) #layer 3\r\n",
        "  tf.keras.layers.Dropout(dp3),\r\n",
        "\r\n",
        " \r\n",
        "  model.add(keras.layers.Dense(1)) #output  \r\n",
        "  \r\n",
        "  model.compile(optimizer = keras.optimizers.Adam(learning_rate = 1e-4),\r\n",
        "                loss = tf.keras.losses.BinaryCrossentropy(),\r\n",
        "                metrics = ['accuracy'])\r\n",
        "  \r\n",
        "  return model "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dsq_auzvwx9"
      },
      "source": [
        "tuner = kt.BayesianOptimization(model_builder,  \n",
        "                     objective='val_accuracy',\n",
        "                     max_trials = 2,\n",
        "                     directory = 'my_dir',\n",
        "                     project_name = 'into_to_kt',\n",
        "                     overwrite=True,\n",
        "                     )"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzHVVciXJbgB"
      },
      "source": [
        "class ClearTrainingOutput(tf.keras.callbacks.Callback):\r\n",
        "  def on_train_end(*args, **kwargs):\r\n",
        "    IPython.display.clear_output(wait = True)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEYR0xLlJfPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9885fee-8bd3-4ffa-a383-5f2a9c4b2a55"
      },
      "source": [
        "tuner.search(padded, labels, epochs = 1, validation_data = (testing_padded, test_labels), batch_size=32, callbacks = [ClearTrainingOutput()])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYvDgSzEN7L-",
        "outputId": "133f8dd8-c682-4d0a-84b3-723e3eabad00"
      },
      "source": [
        "# Get the optimal hyperparameters\r\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\r\n",
        "\r\n",
        "for i in range(1,4):\r\n",
        "   print(best_hps.get(f\"units{i}\"))\r\n",
        "   print(best_hps.get(f\"dp{i}\"))\r\n",
        "   \r\n",
        "#tuner.results_summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n",
            "0.5\n",
            "128\n",
            "0.25\n",
            "448\n",
            "0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIF9u-RPGhBX"
      },
      "source": [
        "Feed the best hyperparameter combination into a final neural network for training 20 epochs and yield predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qscJC5hZjgoV"
      },
      "source": [
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True),\r\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(best_hps.get(\"units1\"), return_sequences=True)),\r\n",
        "    tf.keras.layers.Dropout(best_hps.get(\"dp1\")),\r\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(best_hps.get(\"units2\"))),\r\n",
        "    tf.keras.layers.Dropout(best_hps.get(\"dp2\")),\r\n",
        "    tf.keras.layers.Dense(best_hps.get(\"units3\"), activation='relu'),\r\n",
        "    tf.keras.layers.Dropout(best_hps.get(\"dp3\")),\r\n",
        "    tf.keras.layers.Dense(1)\r\n",
        "])\r\n",
        "\r\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\r\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=opt ,metrics=[\"accuracy\"])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hNu3tHRNIZG"
      },
      "source": [
        "This is the best hyperparameter model\r\n",
        "```\r\n",
        "model = tf.keras.Sequential([\r\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\r\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(512,return_sequences=True)),\r\n",
        "    tf.keras.layers.Dropout(0.5),\r\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\r\n",
        "    tf.keras.layers.Dropout(0.5),\r\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\r\n",
        "    tf.keras.layers.Dropout(0.5),\r\n",
        "    tf.keras.layers.Dense(1)\r\n",
        "])\r\n",
        "\r\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\r\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),optimizer=opt ,metrics=[\"accuracy\"])\r\n",
        "```\r\n",
        "\r\n",
        "Train the model with `batch_size=32`\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW4JeqDXntgn",
        "outputId": "e052ff0a-aa25-49bf-d904-90d0b9f0ae99"
      },
      "source": [
        "num_epochs = 20\r\n",
        "history = model.fit(padded, labels, epochs=num_epochs, validation_data=(testing_padded, test_labels), batch_size=32 ,verbose=1, callbacks =[callback])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "455/455 [==============================] - 25s 41ms/step - loss: 0.0876 - accuracy: 0.9938 - val_loss: 0.6821 - val_accuracy: 0.9382\n",
            "Epoch 2/10\n",
            "455/455 [==============================] - 14s 32ms/step - loss: 0.0868 - accuracy: 0.9941 - val_loss: 0.6869 - val_accuracy: 0.9390\n",
            "Epoch 3/10\n",
            "455/455 [==============================] - 15s 34ms/step - loss: 0.0868 - accuracy: 0.9942 - val_loss: 0.6961 - val_accuracy: 0.9365\n",
            "Epoch 4/10\n",
            "455/455 [==============================] - 15s 33ms/step - loss: 0.0867 - accuracy: 0.9942 - val_loss: 0.7127 - val_accuracy: 0.9365\n",
            "Epoch 5/10\n",
            "455/455 [==============================] - 15s 32ms/step - loss: 0.0882 - accuracy: 0.9939 - val_loss: 0.7098 - val_accuracy: 0.9379\n",
            "Epoch 6/10\n",
            "455/455 [==============================] - 15s 34ms/step - loss: 0.0877 - accuracy: 0.9942 - val_loss: 0.7196 - val_accuracy: 0.9398\n",
            "Epoch 7/10\n",
            "455/455 [==============================] - 15s 32ms/step - loss: 0.0874 - accuracy: 0.9943 - val_loss: 0.7221 - val_accuracy: 0.9393\n",
            "Epoch 8/10\n",
            "455/455 [==============================] - 15s 33ms/step - loss: 0.0875 - accuracy: 0.9942 - val_loss: 0.7462 - val_accuracy: 0.9395\n",
            "Epoch 9/10\n",
            "455/455 [==============================] - 15s 32ms/step - loss: 0.1159 - accuracy: 0.9919 - val_loss: 0.8420 - val_accuracy: 0.9288\n",
            "Epoch 10/10\n",
            "455/455 [==============================] - 15s 33ms/step - loss: 0.0897 - accuracy: 0.9936 - val_loss: 0.7655 - val_accuracy: 0.9398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FReqLBaGzHw"
      },
      "source": [
        "#model.save('/_Pneu3.h5') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYQ9wON36r85"
      },
      "source": [
        "Load Most Accurate Model after many \"tuning sessions\" with keras tuner:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "id": "A3i5Acl4yrHZ"
      },
      "source": [
        "model = keras.models.load_model('/_Pneu3.h5')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzfshPgdzATE",
        "outputId": "de5ca8b0-6b52-4007-c323-2256b97f9165"
      },
      "source": [
        "model.evaluate(x=testing_padded, y=test_labels, batch_size=10)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "364/364 [==============================] - 7s 8ms/step - loss: 0.6535 - accuracy: 0.9412\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6535272002220154, 0.9411764740943909]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    }
  ]
}